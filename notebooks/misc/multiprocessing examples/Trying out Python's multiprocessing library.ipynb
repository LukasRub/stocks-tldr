{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d641b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/anaconda3/envs/tldr/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from multiprocessing import Lock, Process, cpu_count\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9152a88c",
   "metadata": {},
   "source": [
    "### `multiprocessing` simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1539219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process [762620]: 0\n",
      "Process [762623]: 1\n",
      "Process [762626]: 2\n",
      "Process [762631]: 3\n",
      "Process [762632]: 4\n",
      "Process [762635]: 5\n",
      "Process [762636]: 6\n",
      "\n",
      "Process [762637]: 7Process [762642]: 8\n",
      "Process [762645]: 9\n",
      "Process [762648]: 10\n",
      "Process [762653]: 11\n",
      "Process [762658]: 12\n",
      "Process [762661]: 13\n",
      "Process [762669]: 15\n",
      "Process [762666]: 14\n",
      "Process [762670]: 16\n",
      "Process [762673]: 17\n",
      "Process [762677]: 19\n",
      "Process [762676]: 18\n",
      "Process [762680]: 20\n",
      "Process [762683]: 21\n",
      "Process [762686]: 22\n",
      "Process [762695]: 23\n",
      "Process [762698]: 24\n"
     ]
    }
   ],
   "source": [
    "lock = Lock()\n",
    "\n",
    "def spawn(num):\n",
    "    with lock:\n",
    "        print(\"Process [{}]: {}\".format(os.getpid(), num))\n",
    "        \n",
    "\n",
    "for i in range(25):\n",
    "    Process(target=spawn, args=(i,)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7e927",
   "metadata": {},
   "source": [
    "### `joblib` simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b37b90c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "[Parallel(n_jobs=4)]: Using backend MultiprocessingBackend with 4 concurrent workers.\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 tasks      | elapsed:    2.1s\n",
      "64\n",
      "81\n",
      "100\n",
      "121\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=4)]: Done   6 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=4)]: Done   7 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=4)]: Done   8 tasks      | elapsed:    4.1s\n",
      "144\n",
      "169\n",
      "196\n",
      "225\n",
      "[Parallel(n_jobs=4)]: Done   9 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=4)]: Done  11 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:    6.1s\n",
      "256\n",
      "289\n",
      "324\n",
      "361\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=4)]: Done  15 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=4)]: Done  16 tasks      | elapsed:    8.1s\n",
      "400\n",
      "441\n",
      "484\n",
      "529\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=4)]: Done  18 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=4)]: Done  19 out of  25 | elapsed:   10.1s remaining:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  25 | elapsed:   10.1s remaining:    2.5s\n",
      "576\n",
      "[Parallel(n_jobs=4)]: Done  21 out of  25 | elapsed:   12.1s remaining:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done  22 out of  25 | elapsed:   12.1s remaining:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done  23 out of  25 | elapsed:   12.1s remaining:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:   14.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:   14.1s finished\n"
     ]
    }
   ],
   "source": [
    "lock = Lock()\n",
    "\n",
    "def f(num):\n",
    "    with lock:\n",
    "        print(num**2)\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "parallel = Parallel(n_jobs=4, backend=\"multiprocessing\", verbose=50, \n",
    "                    batch_size=1, max_nbytes=None, mmap_mode=None)\n",
    "\n",
    "_ = parallel(delayed(f)(i) for i in range(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13862c8",
   "metadata": {},
   "source": [
    "### `multiprocessing` applicable example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087820b7",
   "metadata": {},
   "source": [
    "#### Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5412fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stocks(stocks):\n",
    "    return [s for s in stocks if s[\"wiki\"] is not None]\n",
    "\n",
    "\n",
    "def read_jsonl_file(filename, processing_func=None):\n",
    "    objects = list()\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as fp:\n",
    "        for line in fp:\n",
    "            obj = json.loads(line)\n",
    "            objects.append(obj)\n",
    "    if processing_func:\n",
    "        objects = processing_func(objects)\n",
    "    return objects\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokenized = [token.lower() for token in word_tokenize(text) if token.isalpha()]\n",
    "    filtered = [token for token in tokenized if not token in stop_words]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def read_and_process_articles(article_paths):\n",
    "    articles = list()\n",
    "    for i, path in enumerate(article_paths):\n",
    "        \n",
    "        # Assering that loop index matches with filename of the article\n",
    "        assert i == int(path.stem)\n",
    "        \n",
    "        with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "            full_txt = fp.read()\n",
    "            \n",
    "        articles.append(tokenize(full_txt))\n",
    "    \n",
    "    return articles\n",
    "        \n",
    "\n",
    "def get_model_str_repr(model, epochs_param_str_marker=\"ep\"):\n",
    "    model_str = str(model).replace(\"/\", \"-\")\n",
    "    epochs = model.epochs\n",
    "    return f\"{model_str[:-1]},{epochs_param_str_marker}{epochs})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e37a84",
   "metadata": {},
   "source": [
    "#### Defining core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f8f35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCK = Lock()\n",
    "\n",
    "def multiproccess_print(status_msg, lock=LOCK):\n",
    "    if lock:\n",
    "        with lock:\n",
    "            print(\"[PID: {}] {}\".format(os.getpid(), status_msg))\n",
    "    else:\n",
    "        print(status_msg)\n",
    "\n",
    "\n",
    "def infer_article_vectors(model, articles, lock=LOCK, vectors_per_article=100, epochs=20):\n",
    "    \n",
    "    # Initializing empty array for article vectors\n",
    "    avs = np.empty((len(articles), vectors_per_article, model.vector_size))\n",
    "    \n",
    "    # Inferring vectors for articles\n",
    "    for i, article in enumerate(articles):\n",
    "        for j in range(vectors_per_article):\n",
    "            status_msg = (\"Inferring vector #{} for article {}.txt with model {}\"\n",
    "                              .format(j+1, i, get_model_str_repr(model)))\n",
    "            multiproccess_print(status_msg, lock)  # Displaying status message\n",
    "            avs[i, j, :] = model.infer_vector(article, epochs=epochs)\n",
    "    \n",
    "    return avs\n",
    "\n",
    "\n",
    "def infer_entity_vectors(model, stock, lock=LOCK, vectors_per_entity=10, epochs=20):\n",
    "    \n",
    "    # Initializing empty arrays for entity vectors\n",
    "    evs_full = np.empty((vectors_per_entity, model.vector_size))\n",
    "    evs_summary = np.empty((vectors_per_entity, model.vector_size))\n",
    "    evs_child = np.empty((len(stock[\"entities\"]), vectors_per_entity, model.vector_size))\n",
    "    \n",
    "    # Displaying status message\n",
    "    status_msg = (\"Inferring entity {} vectors with model {}\"\n",
    "                      .format(stock[\"ticker\"], get_model_str_repr(model)))\n",
    "    multiproccess_print(status_msg, lock)\n",
    "    \n",
    "    # Inferring vectors for the parent entity\n",
    "    for i in range(vectors_per_entity):\n",
    "        evs_full[i, :] = model.infer_vector(stock[\"content\"], epochs=epochs)\n",
    "        evs_summary[i, :] = model.infer_vector(stock[\"summary\"], epochs=epochs)\n",
    "        \n",
    "    # Inferring vectors for child entities\n",
    "    for i, child_entity in enumerate(stock[\"entities\"]):\n",
    "        for j in range(vectors_per_entity):\n",
    "            evs_child[i, j, :] = model.infer_vector(child_entity[\"summary\"], epochs=epochs)\n",
    "    \n",
    "    return evs_full, evs_summary, evs_child\n",
    "\n",
    "            \n",
    "def infer_vectors(articles, stocks, model_path, lock=LOCK, vectors_per_article=100, \n",
    "                  vectors_per_entity=10, epochs=20):\n",
    "    # Load model\n",
    "    model = Doc2Vec.load(str(model_path))\n",
    "    model_str_repr = get_model_str_repr(model)\n",
    "    \n",
    "    # Set up directories for article vectors\n",
    "    avs_filename = (\"vpa{vpa}.ep{ep}.av.vectors.npy\"\n",
    "                        .format(vpa=vectors_per_article, ep=epochs))\n",
    "    avs_path = (Path(PATH_TO_RESULTS) \n",
    "                   / Path(\"multiprocess\" if lock else \"single_process\")\n",
    "                   / Path(model_str_repr)  \n",
    "                   / Path(ARTICLE_VECTORS_DIR)\n",
    "                   / avs_filename)\n",
    "    \n",
    "    # Infer article vectors and save them to a file if they don't already exist\n",
    "    if not avs_path.exists():\n",
    "        avs_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        avs = infer_article_vectors(model, articles, lock=lock, \n",
    "                                    vectors_per_article=vectors_per_article, \n",
    "                                    epochs=epochs)\n",
    "        multiproccess_print((\"Saving article vectors for {} to {}...\"\n",
    "                                 .format(model_str_repr, avs_path)), lock)\n",
    "        np.save(avs_path, avs)\n",
    "    else:\n",
    "        multiproccess_print(f\"Article vectors for {model_str_repr} already exist.\", lock)\n",
    "    \n",
    "    for stock in stocks:\n",
    "        # Set up directories for entity vectors\n",
    "        entity = stock[\"ticker\"]\n",
    "        evs_filename = (\"{ticker}.vpe{vpe}.ep{ep}.ev.vectors.npy\"\n",
    "                            .format(ticker=entity, vpe=vectors_per_entity, ep=epochs))\n",
    "        evs_path = (Path(PATH_TO_RESULTS) \n",
    "                       / Path(\"multiprocess\" if lock else \"single_process\")\n",
    "                       / Path(model_str_repr)  \n",
    "                       / Path(ENTITY_VECTORS_DIR)\n",
    "                       / evs_filename)\n",
    "        \n",
    "        # Infer entity vectors and save them to a file if they don't already exist\n",
    "        if not Path(str(evs_path) + \".npz\").exists(): # NumPy array archives have additional .npy suffix\n",
    "            evs_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            evs_full, evs_summary, evs_child = infer_entity_vectors(model, stock, lock=lock, \n",
    "                                                                    vectors_per_entity=vectors_per_entity,\n",
    "                                                                    epochs=epochs)\n",
    "            multiproccess_print((\"Saving entity {} vectors for {} to {}...\"\n",
    "                                     .format(entity, model_str_repr, evs_path)), lock)\n",
    "            np.savez(evs_path, evs_full=evs_full, evs_summary=evs_summary, evs_child=evs_child)\n",
    "        else:\n",
    "            multiproccess_print(f\"Entity {entity} vectors for {model_str_repr} already exist.\", lock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576605b",
   "metadata": {},
   "source": [
    "#### Setting up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf2b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_STOCKS = \"./stocks/\"\n",
    "PATH_TO_MODELS = \"./models/\"\n",
    "PATH_TO_ARTICLES = \"./articles/\"\n",
    "PATH_TO_RESULTS = \"./results/\"\n",
    "ARTICLE_VECTORS_DIR = \"article_vectors\"\n",
    "ENTITY_VECTORS_DIR = \"entity_vectors\"\n",
    "\n",
    "stocks_path = list(Path(PATH_TO_STOCKS).glob(\"*.jsonl\"))[0]\n",
    "model_paths = [path for path in Path(PATH_TO_MODELS).glob(\"*/*\")\n",
    "               if path.suffix == \"\"]\n",
    "article_paths = sorted(list(Path(\"./articles/\").glob(\"*.txt\")), \n",
    "                       key=lambda x: int(x.stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f5d92",
   "metadata": {},
   "source": [
    "#### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7263a14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stocks = read_jsonl_file(stocks_path)\n",
    "articles = read_and_process_articles(article_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0129e",
   "metadata": {},
   "source": [
    "#### Without `multiprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3476eb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PID: 762600] Article vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Article vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Article vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Article vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 762600] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "Single CPU core bound task performance: 10s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "for model_path in model_paths:\n",
    "    infer_vectors(articles[:1], stocks[:3], model_path, vectors_per_article=10)\n",
    "    \n",
    "t1 = time.time() - t0\n",
    "print(\"Single CPU core bound task performance: {:.0f}s\".format(t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9383865",
   "metadata": {},
   "source": [
    "#### With `multiprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18de3587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PID: 762600] Process [PID:763268] started\n",
      "[PID: 762600] Process [PID:763269] started\n",
      "[PID: 762600] Process [PID:763270] started\n",
      "[PID: 762600] Process [PID:763271] started\n",
      "[PID: 763270] Article vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763270] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763271] Article vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763270] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763271] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763271] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763271] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763270] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763269] Article vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763269] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763269] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763269] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763268] Article vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763268] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763268] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763268] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.\n",
      "Multiple CPU core bound task performance: 3s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "lock = Lock()\n",
    "processes = []\n",
    "\n",
    "for model_path in model_paths:\n",
    "    p = Process(target=infer_vectors, args=(articles[:1], stocks[:3], model_path),\n",
    "                kwargs=dict(lock=lock, vectors_per_article=10))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "    multiproccess_print(\"Process [PID:{}] started\".format(p.pid))\n",
    "    \n",
    "for p in processes:\n",
    "    p.join()\n",
    "    \n",
    "t1 = time.time() - t0\n",
    "print(\"Multiple CPU core bound task performance: {:.0f}s\".format(t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd410b2b",
   "metadata": {},
   "source": [
    "#### Checking for side effects\n",
    "The purpose of this check is to see wether using `multiprocessing` affected inferred vectors in any way comparing them to the same vectors computer linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return np.einsum('ij,ij->i', v1, v2) / (np.linalg.norm(v1, axis=1)*np.linalg.norm(v2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecad3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avs - article vectors\n",
    "avs_single_paths = sorted(list((Path(PATH_TO_RESULTS) / \"single_process\").glob(\"*/article_vectors/*.npy\")),\n",
    "                         key=lambda x: int(re.search(\"w\\d{1}\", str(x)).group()[1:]))\n",
    "avs_multi_paths = sorted(list((Path(PATH_TO_RESULTS) / \"multiprocess\").glob(\"*/article_vectors/*.npy\")),\n",
    "                        key=lambda x: int(re.search(\"w\\d{1}\", str(x)).group()[1:]))\n",
    "\n",
    "\n",
    "for paths in zip(avs_single_paths, avs_multi_paths):\n",
    "    assert paths[0].parent.parent.name == paths[1].parent.parent.name\n",
    "    model_str = print(paths[0].parent.parent.name)\n",
    "    \n",
    "    avs_single = np.load(paths[0])[0, :, :]\n",
    "    avs_multi = np.load(paths[1])[0, :, :]\n",
    "    \n",
    "    mean_similarity = cosine_similarity(avs_single, avs_multi).mean()\n",
    "    print(\"Mean cosine similarity between articles:\", mean_similarity,  \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b46e090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evs_single_paths = sorted(list((Path(PATH_TO_RESULTS) / \"single_process\").glob(\"*/entity_vectors/*.npz\")),\n",
    "                         key=lambda x: int(re.search(\"w\\d{1}\", str(x)).group()[1:]))\n",
    "evs_multi_paths = sorted(list((Path(PATH_TO_RESULTS) / \"multiprocess\").glob(\"*/entity_vectors/*.npz\")),\n",
    "                        key=lambda x: int(re.search(\"w\\d{1}\", str(x)).group()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f35531cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for paths in zip(evs_single_paths, evs_multi_paths):\n",
    "    assert paths[0].parent.parent.name == paths[1].parent.parent.name\n",
    "    assert paths[0].name.split(\".\")[0] == paths[1].name.split(\".\")[0]\n",
    "    \n",
    "    model_str = paths[0].parent.parent.name\n",
    "    entity = paths[0].name.split(\".\")[0]\n",
    "    \n",
    "    print(model_str, entity)\n",
    "    \n",
    "    npzfile_single = np.load(paths[0])\n",
    "    npzfile_multi = np.load(paths[1])\n",
    "    \n",
    "    evs_full_single = npzfile_single[\"evs_full\"]\n",
    "    evs_summary_single = npzfile_single[\"evs_summary\"]\n",
    "    evs_summary_child = npzfile_single[\"evs_child\"]\n",
    "    \n",
    "    evs_full_multi = npzfile_multi[\"evs_full\"]\n",
    "    evs_summary_multi = npzfile_multi[\"evs_summary\"]\n",
    "    evs_summary_multi = npzfile_multi[\"evs_child\"]\n",
    "    \n",
    "    for key in [\"evs_full\", \"evs_summary\", \"evs_child\"]:\n",
    "        evs_single = npzfile_single[key]\n",
    "        evs_multi = npzfile_multi[key]\n",
    "        if key == \"evs_child\":\n",
    "            evs_single = evs_single.reshape((evs_single.shape[0]*evs_single.shape[1],\n",
    "                                             evs_single.shape[2]))\n",
    "            evs_multi = evs_multi.reshape((evs_multi.shape[0]*evs_multi.shape[1],\n",
    "                                             evs_multi.shape[2]))\n",
    "        similarities = cosine_similarity(evs_single, evs_multi)\n",
    "        print(f\"{key} mean cosine similarity {similarities.mean()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9644951",
   "metadata": {},
   "source": [
    "There are negligible differences between similarities as expected, due to the random vector initialization nature of Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035fd16",
   "metadata": {},
   "source": [
    "###  `joblib` applicable example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9534d9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend MultiprocessingBackend with 4 concurrent workers.\n",
      "[PID: 763340] Article vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763340] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763340] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763340] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w2,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763338] Article vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.\n",
      "\n",
      "[PID: 763338] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.[PID: 763338] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763338] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w4,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763341] Article vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763341] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763341] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763341] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w1,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763339] Article vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763339] Entity AMD vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763339] Entity AA vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.\n",
      "[PID: 763339] Entity BABA vectors for Doc2Vec(dm-c,d100,n20,w3,mc5,s1e-05,t4,ep20) already exist.[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    3.3s remaining:    3.3s\n",
      "\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    3.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    3.5s finished\n",
      "Multiple CPU core bound task performance: 4s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "parallel = Parallel(n_jobs=4, backend=\"multiprocessing\", verbose=50, \n",
    "                    batch_size=1, max_nbytes=None, mmap_mode=None)\n",
    "\n",
    "partial_infer_vectors = partial(infer_vectors, articles[:1], stocks[:3], vectors_per_article=10)\n",
    "parallel(delayed(partial_infer_vectors)(model_path) for model_path in model_paths)\n",
    "    \n",
    "    \n",
    "t1 = time.time() - t0\n",
    "print(\"Multiple CPU core bound task performance: {:.0f}s\".format(t1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('tldr': conda)",
   "language": "python",
   "name": "python396jvsc74a57bd0e4c6f9ab0afc44c5c63e1b233fc90c970e43406a17ee57d06d7c91febfb3a0f7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
